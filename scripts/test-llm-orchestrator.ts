#!/usr/bin/env npx tsx
// scripts/test-llm-orchestrator.ts
// Phase 170 Test: Multi-Model Orchestrator Testing

import {
  LLMRouter,
  getModelConfig,
  LLM_MODELS,
  LLMClientFactory,
  instrumentedLLMCall,
  BenchmarkEngine,
  getModelComparisons,
} from '../orchestrator/core/llm';

async function main() {
  console.log('\nüß™ Phase 170 - Multi-Model Orchestrator Tests\n');
  console.log('‚ïê'.repeat(60));

  // Test 1: Model Registry
  console.log('\nüìã Test 1: Model Registry');
  console.log('-'.repeat(40));
  console.log(`Total models registered: ${LLM_MODELS.length}`);

  const providers = [...new Set(LLM_MODELS.map(m => m.provider))];
  console.log(`Providers: ${providers.join(', ')}`);

  for (const provider of providers) {
    const models = LLM_MODELS.filter(m => m.provider === provider);
    console.log(`  ${provider}: ${models.map(m => m.id).join(', ')}`);
  }
  console.log('‚úÖ Model Registry OK\n');

  // Test 2: Router - Free Tier
  console.log('üìã Test 2: Router (Free Tier)');
  console.log('-'.repeat(40));

  const freeAutoFix = LLMRouter.routeCodeTask('AUTO_FIX', 'free');
  console.log(`AUTO_FIX ‚Üí ${freeAutoFix.preferredModel}`);
  console.log(`  Reason: ${freeAutoFix.reason}`);
  console.log(`  Fallbacks: ${freeAutoFix.fallbackModels.join(', ')}`);

  const freeChat = LLMRouter.routeChatTask('free');
  console.log(`CHAT ‚Üí ${freeChat.preferredModel}`);
  console.log('‚úÖ Free Tier Routing OK\n');

  // Test 3: Router - Pro Tier
  console.log('üìã Test 3: Router (Pro Tier)');
  console.log('-'.repeat(40));

  const proAutoFix = LLMRouter.routeCodeTask('AUTO_FIX', 'pro');
  console.log(`AUTO_FIX ‚Üí ${proAutoFix.preferredModel}`);
  console.log(`  Reason: ${proAutoFix.reason}`);

  const proCodeReview = LLMRouter.routeCodeTask('CODE_REVIEW', 'pro');
  console.log(`CODE_REVIEW ‚Üí ${proCodeReview.preferredModel}`);

  const proChat = LLMRouter.routeChatTask('pro');
  console.log(`CHAT ‚Üí ${proChat.preferredModel}`);
  console.log('‚úÖ Pro Tier Routing OK\n');

  // Test 4: Router - Ultimate Tier
  console.log('üìã Test 4: Router (Ultimate Tier)');
  console.log('-'.repeat(40));

  const ultAutoFix = LLMRouter.routeCodeTask('AUTO_FIX', 'ultimate');
  console.log(`AUTO_FIX ‚Üí ${ultAutoFix.preferredModel}`);

  const ultCodeReview = LLMRouter.routeCodeTask('CODE_REVIEW', 'ultimate');
  console.log(`CODE_REVIEW ‚Üí ${ultCodeReview.preferredModel}`);
  console.log('‚úÖ Ultimate Tier Routing OK\n');

  // Test 5: Model Comparisons
  console.log('üìã Test 5: Model Comparisons');
  console.log('-'.repeat(40));

  const comparisons = getModelComparisons();
  console.log('Model comparison data:');
  for (const c of comparisons.slice(0, 4)) {
    console.log(`  ${c.label}: ${c.costTier}/${c.qualityTier}/${c.speedTier} ‚Üí ${c.bestFor.join(', ')}`);
  }
  console.log('‚úÖ Comparisons OK\n');

  // Test 6: Check Available Providers
  console.log('üìã Test 6: Available API Keys');
  console.log('-'.repeat(40));

  const apiKeys = {
    OPENAI: !!process.env.OPENAI_API_KEY,
    ANTHROPIC: !!process.env.ANTHROPIC_API_KEY,
    MISTRAL: !!process.env.MISTRAL_API_KEY,
    DEVSTRAL: !!process.env.DEVSTRAL_API_KEY || !!process.env.MISTRAL_API_KEY,
    GEMINI: !!process.env.GOOGLE_AI_API_KEY || !!process.env.GEMINI_API_KEY,
  };

  for (const [provider, available] of Object.entries(apiKeys)) {
    console.log(`  ${provider}: ${available ? '‚úÖ Available' : '‚ùå Missing'}`);
  }
  console.log('');

  // Test 7: Live API Call (if OpenAI available)
  if (apiKeys.OPENAI) {
    console.log('üìã Test 7: Live API Call (OpenAI)');
    console.log('-'.repeat(40));

    try {
      const result = await instrumentedLLMCall({
        taskType: 'CHAT',
        userTier: 'ultimate', // Higher tier to avoid cost downgrade
        userId: 'test-user',
        messages: [
          { role: 'system', content: 'You are a helpful assistant. Be very brief.' },
          { role: 'user', content: 'Say "Phase 170 test successful!" in exactly those words.' },
        ],
        temperature: 0,
        maxTokens: 50,
        forceModel: 'gpt-4o-mini', // Force OpenAI since we know it's available
        excludeProviders: ['gemini', 'anthropic', 'mistral', 'devstral'], // Only use OpenAI
      });

      console.log(`  Success: ${result.success}`);
      console.log(`  Model: ${result.model}`);
      console.log(`  Provider: ${result.provider}`);
      console.log(`  Latency: ${result.metrics.latencyMs}ms`);
      console.log(`  Tokens: ${result.metrics.inputTokens} in / ${result.metrics.outputTokens} out`);
      console.log(`  Cost: $${result.metrics.estimatedCostUSD?.toFixed(6) || 'N/A'}`);
      console.log(`  Response: "${result.content.slice(0, 100)}..."`);
      console.log('‚úÖ Live API Call OK\n');
    } catch (err: any) {
      console.log(`  ‚ùå Error: ${err.message}`);
      console.log('');
    }
  }

  // Test 8: Benchmark Stats
  console.log('üìã Test 8: Benchmark Engine');
  console.log('-'.repeat(40));

  const summary = BenchmarkEngine.getDashboardSummary();
  console.log(`  Total runs: ${summary.totalRuns}`);
  console.log(`  Success rate: ${(summary.successRate * 100).toFixed(1)}%`);
  console.log(`  Avg latency: ${summary.avgLatencyMs.toFixed(0)}ms`);
  console.log(`  Total cost: $${summary.totalCostUSD.toFixed(6)}`);
  console.log('‚úÖ Benchmark Engine OK\n');

  // Summary
  console.log('‚ïê'.repeat(60));
  console.log('üìä Phase 170 Test Summary');
  console.log('‚ïê'.repeat(60));
  console.log(`
‚úÖ Model Registry: ${LLM_MODELS.length} models across ${providers.length} providers
‚úÖ Router: Tier-based routing working
‚úÖ Cost Optimizer: Budget controls ready
‚úÖ Benchmark Engine: Performance tracking ready
‚úÖ ACE Integration: Ready for Auto-Fix tasks

üîë API Keys Status:
   - OpenAI: ${apiKeys.OPENAI ? '‚úÖ' : '‚ùå'}
   - Mistral/DevStral: ${apiKeys.MISTRAL ? '‚úÖ' : '‚ùå (Add MISTRAL_API_KEY)'}
   - Anthropic: ${apiKeys.ANTHROPIC ? '‚úÖ' : '‚ùå (Optional)'}
   - Gemini: ${apiKeys.GEMINI ? '‚úÖ' : '‚ùå (Optional)'}

${!apiKeys.MISTRAL ? `
‚ö†Ô∏è  To enable DevStral for code tasks, add to .env.local:
   MISTRAL_API_KEY=your-mistral-api-key

   Get your key from: https://console.mistral.ai/
` : ''}
`);
}

main().catch(console.error);
